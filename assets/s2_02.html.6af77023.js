import{_ as l}from"./gzh.b03adfd2.js";import{_ as i}from"./zsxq.77f6dc86.js";import{_ as c}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as r,c as u,a as n,d as s,b as a,w as d,e,r as p}from"./app.571c9ec8.js";const k="/assets/1596515444493-d5cc3586-8811-4d98-8fee-cba5044aeb1e.c07f8d01.png",m="/assets/1596515444616-5c225efe-0cce-43a7-903d-672629aee031.8ec63ef2.png",b="/assets/1596515444479-2aa8f9ab-e5f9-4dc9-a2d7-760843ac458c.a2f11272.png",v="/assets/1596515444715-5e67b4fa-ce0f-479e-8fcc-2bd75a9fc4cd.b772410f.png",g="/assets/1596515444487-e537acd3-43fd-4095-b391-c1bf9383e23d.0c29056d.png",h="/assets/1596515444616-4231b9eb-4cf1-4483-b315-838bd0c15904.583f2ba8.png",f="/assets/1596515444620-859d3f02-ec50-45ec-994f-21d0b4110ac9.533aedfe.png",_="/assets/1596515444572-79c371fa-3409-4dfa-aa92-f80fab19ab1e.58af17ff.png",y="/assets/1596515444528-5458d5db-99fc-4dfe-9f1a-1d1d16138c00.ecccac93.png",w="/assets/1596515444512-c78302e8-aa53-4536-b1a9-4156c3562323.33a3026b.png",x="/assets/1596515444581-7d231bf6-d464-4a94-8f97-4b8b259e9e87.c623b9f7.png",q="/assets/1596515444613-34fd2661-7b03-4d37-99e1-0d1974c492a7.316f37ed.png",z="/assets/1596515444586-2347df9b-0238-4323-b1c6-89a35e9b0cbf.6557b9f2.png",P="/assets/1596515444515-8ab7ab7e-5e5a-4b3a-b01d-0b1c35d53e48.1a9bb9bb.png",R="/assets/1596515444489-7fbd6313-9547-4399-a933-725beae1daf5.49ba240d.png",I="/assets/1596515444765-ce01581c-a080-4500-a5e3-2069db7474fd.edac6139.png",B="/assets/1596515444643-4d627d84-7487-4432-a0e8-906d9d6e626c.cca73b31.png",E="/assets/1596515444690-8dfee9f8-6a71-4e50-87fa-63f61d253e4e.a6981f45.png",L="/assets/1596515444651-b274be85-fe4b-4f92-b430-6769e7cef333.f0800570.png",U="/assets/1596515444835-e8caad2c-192a-40b2-8fcd-b535b71c4765.e6f5bf27.png",A="/assets/1596515444803-7ebcd2d5-d8d6-44ac-a5dd-7ac3a2ee9cc3.7689fd96.png",D="/assets/1596515444688-ee93b125-03b3-489d-afec-93636a742737.3ec818f2.png",O="/assets/1596515444804-1767c101-be1d-42c9-bf13-4ddf5a4b6546.4762a153.png",S="/assets/1596515444672-573249cb-7aa8-4ca9-aeed-0002a40d390c.ea557cac.png",T="/assets/1596515444756-cc47fd46-46ff-4029-9b48-458cee09b90e.b03a1190.png",C="/assets/1596515444679-21479a5e-03e8-4023-adc3-54bdd15a2bb5.76617ab5.png",j="/assets/1596515444722-273e783b-11cb-47c6-9433-e4dd27f500c5.3e8a0739.png",W="/assets/1596515444670-77a8b9b6-f247-4b57-bfe4-7ebb16a4ce36.6bf37c00.png",F="/assets/1596515444757-07a93687-9163-4908-b1a9-4697134811b6.b7696224.png",Q="/assets/1596515444672-b29b783a-266e-4939-865f-63c448d81e17.f2240938.png",G="/assets/1596515444703-4c170df6-52a3-4558-a49f-37c5db020a21.22077bf0.png",J="/assets/1596515444716-9daab4d1-473b-4990-8c88-09715719c48d.285a7253.png",V="/assets/1596515444770-6dfd5487-a4c9-4e17-b051-65d870297d0a.f72e80d3.png",N="/assets/1596515444860-043ae329-582d-423b-adf8-77f19adf90b5.db54c154.png",H="/assets/1596515444855-5daa9fde-b6d9-4966-b826-07b2f32fdeb0.c8dcef94.png",M={},$=e('<p>你好，我是悦创。</p><p>互联网诞生之初，是为了让人们更容易的分享数据、交流通讯。互联网是桥梁，连接了世界各地的人们。网站的点击、浏览都是人为的，与你聊天的也是活生生的人。</p><p>然而，随着技术的发展，人们对数据的渴望，出现了各种网络机器人，这个时候，你不知道屏幕那端跟你聊天的是一个人还是一条狗，你也不知道你网站的浏览量是人点击出来的，还是机器爬出来的。</p><p><img src="'+k+'" alt="" loading="lazy"></p><hr><p>表面上看，互联网上是各种各样的人；暗地里，已经布满了形形色色的网络爬虫。</p><p>在讲什么是爬虫之前呢，我来讲讲爬虫的职位问题，这也是很多同学报名我的爬虫私教课的初心之一。</p><h2 id="_1-大数据时代的爬虫岗位" tabindex="-1"><a class="header-anchor" href="#_1-大数据时代的爬虫岗位" aria-hidden="true">#</a> 1. 大数据时代的爬虫岗位</h2><p><img src="'+m+'" alt="" loading="lazy"></p><p>上面的图片数据，就是为了提高大家对于学习网络爬虫兴趣。可见，爬虫工程师的工资不低鸭！</p><h2 id="_2-网络爬虫概况" tabindex="-1"><a class="header-anchor" href="#_2-网络爬虫概况" aria-hidden="true">#</a> 2. 网络爬虫概况</h2><blockquote><p>当今最大的网络是互联网，最大的爬虫就是各类搜索引擎，包括谷歌、百度等。网络爬虫就是按照一定规则去爬取人类所需要的信息的程序，主要通过对 URL 的请求来实现。 一般来说，从搜索引擎这类爬虫搜索到的信息是非常宽泛的，而且夹杂着各种广告，信息是不纯粹的，也有可能不是我们需要的。 这种时候，就需要一些聚焦于某一方面信息的爬虫来为我们服务，比方说，专门爬取某一类书的信息，在网站上浏览的时候，可能会有形形色色的图片和一些其他信息干扰我们，如果编写网络爬虫的话，就可以单单把自己想要的信息存储下来，以便数据分析，提取有用的信息。</p></blockquote><p><img src="'+b+'" alt="" loading="lazy"></p><p>像我们平时用的，谷歌浏览器、火狐浏览器、IE浏览器等，这些浏览器不是搜索引擎哦，主要的作用是：<strong>渲染我们的这些网页。</strong></p><p>其实，这个浏览器和我们平时写的小爬虫最大的区别就是：我们平时的小爬虫抓取的网页源代码的不渲染的，直接把网页的源代码 <strong>HTML</strong> 展现出来，而浏览器是会加载 <strong>HTML</strong> 相关的 <strong>CSS、JS</strong> 等。爬虫是不会执行这些关联的文件。</p><h2 id="_3-搜索引擎时代的网络爬虫" tabindex="-1"><a class="header-anchor" href="#_3-搜索引擎时代的网络爬虫" aria-hidden="true">#</a> 3. 搜索引擎时代的网络爬虫</h2><p>关于网络爬虫的概念，我们先来瞅瞅维基百科（Wikipedia）上面的定义：</p><blockquote><p>网络爬虫（英语：web crawler），也叫网上蜘蛛（spider），是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引。</p></blockquote><p>这里提到的编纂网络索引，就是搜索引擎干的事情。我们对搜索引擎并不陌生，Google、百度等搜索引擎可能每天都在帮我们快速获得信息。可能同学们要问，搜索引擎的工作过程是怎样的呢？</p><p>首先，就是有网络爬虫不断抓取各个网站的网页，存放到搜索引擎的数据库；</p><p>接着，索引程序读取数据库的网页进行清理，建立倒排索引；</p><p>最后，搜索程序接收用户的查询关键词，去索引里面找到相关内容，并通过一定的排序算法（Pagerank等）把最相关最好的结果排在最前面呈现给用户。「吐槽一下：百度并不是把相关最好的结果排在最前面，而是广告比较多 ！」</p><p>看上去简简单单的三个部分，却构成了强大复杂的搜索引擎系统。而网络爬虫是其中最基础也很重要的一部分，它决定着搜索引擎数据的完整性和丰富性。我们也看到网络爬虫的主要作用是获取数据。</p><p>由此简单地说，网络爬虫就是获取互联网公开数据的自动化工具。</p><p>这里要强调一下，网络爬虫爬取的是互联网上的<strong>公开数据</strong>，而不是通过特殊技术非法入侵到网站服务器获取的非公开数据。</p><p>那么同学们可能要问，什么是 <strong>“公开数据”</strong> 呢？</p><blockquote><p>简而言之，就是网站上公开让用户浏览、获取的数据。</p></blockquote><p>虽然数据是公开的，但是当某人或机构（如，搜索引擎）大量收集这些数据并因此获利时，也会让数据生产方——网站很不爽，由此而产生法律纠纷。比如，早些年 Google 因此而惹上官司。</p><p>网站们看着搜索引擎，因为搜索引擎抓取自己的内容而获利不爽，但也因为搜索引擎带来的流量而高兴不已，于是就出现了网站主动进行搜索引擎优化（SEO, Search Engine Optimization），也就是告诉搜索引擎，我这里的内容好，快来抓取吧！</p><p><img src="'+v+'" alt="" loading="lazy"></p><p>搜索引擎和网站的博弈，催生了一个君子协议： <strong>robots.txt</strong> 。网站在自己的网站上放上这个文件，告诉爬虫哪些内容可以抓，哪些内容不可以抓；</p><p>搜索引擎读取网站的 <strong>robots.txt</strong> 来知道自己的抓取范围，同时也在访问网站时通过 <strong>User-Agent</strong> 来向网站表明自己的身份（这种表明也是君子协议，技术上很容易假扮他人），比如，Google 的爬虫叫做 Googlebot，百度的爬虫叫做 Baiduspider。这样，二者和平共处，互惠互利。</p><p><img src="'+g+'" alt="" loading="lazy"></p><h3 id="_3-1-使用-robots-txt" tabindex="-1"><a class="header-anchor" href="#_3-1-使用-robots-txt" aria-hidden="true">#</a> 3.1 使用 robots.txt</h3><p><img src="'+h+`" alt="" loading="lazy"></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> urllib <span class="token keyword">import</span> robotparser

rp <span class="token operator">=</span> robotparser<span class="token punctuation">.</span>RobotFileParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
rp<span class="token punctuation">.</span>set_url<span class="token punctuation">(</span><span class="token string">&#39;https://www.baidu.com/robots.txt&#39;</span><span class="token punctuation">)</span>
url <span class="token operator">=</span> <span class="token string">&#39;https://www.baidu.com/&#39;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>rp<span class="token punctuation">.</span>can_fetch<span class="token punctuation">(</span><span class="token string">&#39;Python&#39;</span><span class="token punctuation">,</span> url<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="_4-大数据时代的网络爬虫" tabindex="-1"><a class="header-anchor" href="#_4-大数据时代的网络爬虫" aria-hidden="true">#</a> 4. 大数据时代的网络爬虫</h2><p>时代在发展，数据变得越来越重要，“大数据”已经成为各行各业讨论的话题，人们对数据的渴望也变成贪婪，数据也就成了“石油”，爬虫也就成了“钻井机”。</p><p>为了获取石油，人们使用钻井机；</p><p>为了获取数据，人们使用爬虫。</p><p>为了获得数据，人们把互联网钻的是“千疮百孔”。</p><p><strong>哈哈，这里有些夸张。</strong></p><p>但人们对数据的获取，已经打破的君子协定，和网站们玩起了猫捉老鼠的游戏，展开了道高一尺魔高一丈的较量。</p><p><strong>为什么说是较量呢？</strong></p><p>因为大量爬虫的行为会给网站带来网络带宽、服务器计算力等方面很大的压力，却几乎不带来任何利益。</p><p>为了降低这种毫无利益的压力和避免自己的数据被他人集中收集，网站肯定要通过技术手段来限制爬虫；另一方面，爬虫为了获取石油般的数据，就想方设法来突破这种限制。</p><p><strong>对于这种较量的理解，还是看活生生的例子来得更透彻。</strong></p><ul><li>你有没有花几十块钱让某个软件帮你抢火车票？ <ul><li>攻： 抢票爬虫会不断访问 12306 来获得火车票座位数据，并进而购买火车票；</li><li>防： 12306 网站出了变态的认证码，人都经常识别错误。</li></ul></li><li>各种秒杀让你很受伤！ <ul><li>攻： 研究网站的秒杀机制，提前写好爬虫，秒杀时刻，人快不过机器；</li><li>防： 有些秒杀的宣传作用很大就懒得防；有些秒杀机制复杂到你很难写出对应的爬虫；有些秒杀成功被发现作弊也会被取消。</li></ul></li></ul><p>爬虫变得越来越多，越来越肆无忌惮，网站也不得不使用各种技术手段来禁止或限制爬虫。这些手段大致包括：</p><ul><li>使用账户保护数据，数据仅对登录用户可见；</li><li>数据多次异步加载；</li><li>限制IP访问频率，甚至封锁 IP；</li><li>输入验证码以获得访问权限；</li><li>数据在服务器端加密，浏览器端解密；</li><li>……</li></ul><p>而这些手段也是爬虫在技术实现中要解决和突破的问题。</p><h2 id="_5-网络爬虫的分类" tabindex="-1"><a class="header-anchor" href="#_5-网络爬虫的分类" aria-hidden="true">#</a> 5. 网络爬虫的分类</h2><p><img src="`+f+'" alt="" loading="lazy"></p><hr><blockquote><p>现在的网络爬虫大体可以分为四类：<strong>全网爬虫，主题爬虫，增量式爬虫和深层爬虫。</strong></p><ol><li>全网爬虫：也就是谷歌百度等搜索引擎，这类爬虫会从一些非常基础的 <strong>URl</strong> 出发，一直延伸到整个网站，这类爬虫庞大臃肿，需要很大的存储空间以及极高的爬行速度。</li><li>主题爬虫：满足特定人的特定需求，不同于全网爬虫，它是有选择地爬行与需求相关的信息。</li><li>增量式爬虫：对已经下载的页面采取增量式更新的爬虫，只爬取新产生的或者发生变化的网页，一定程度上可以保证爬取得页面是最新的，减少了空间的浪费，但是<strong>复杂度较高。</strong></li><li>深层网络爬虫：不能通过静态 <strong>URL</strong> 访问，隐藏在表单后，只有用户提交一些关键词才能得到的页面。</li></ol></blockquote><p><strong>PS: 深层网络爬虫也就是我们所说的暗网，我们一般接触到就是一些 POST 表单，登录页面之类的，你输入账户密码之后，点击登录。这就是一个深层网络爬虫。</strong></p><p><strong>为啥子呢？因为我们的爬虫是自动爬不到里面的，必须设置一些参数才是可以的。</strong></p><p><strong>举个例子：比如我建立了一个网络爬虫。要求它从百度开始遍历所有网站，它遇到有验证码的有登录的就进不去了。因为，它之后的页面是隐藏在表单之后，这个东西就叫网络爬虫（它也是归入暗网的范畴）</strong></p><p><strong>咱们来个简单的介绍暗网：</strong></p>',59),X={href:"https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C/1717974",target:"_blank",rel:"noopener noreferrer"},Y={href:"https://baike.baidu.com/item/%E8%B6%85%E9%93%BE%E6%8E%A5/97857",target:"_blank",rel:"noopener noreferrer"},K={href:"https://baike.baidu.com/item/%E6%B7%B1%E7%BD%91/22089976",target:"_blank",rel:"noopener noreferrer"},Z=n("strong",null,"----------来源----------《百度百科》",-1),nn=e('<p>其实，多说一句，暗网并不难，使用<strong>洋葱路由器</strong>就可以登陆。</p><hr><h3 id="_5-1-增量式爬虫-爬虫策略" tabindex="-1"><a class="header-anchor" href="#_5-1-增量式爬虫-爬虫策略" aria-hidden="true">#</a> 5.1 增量式爬虫 <strong>(爬虫策略)</strong></h3><p><img src="'+_+`" alt="" loading="lazy"></p><hr><p><strong>举个例子：</strong></p><p>还是以微博为例，微博上有个热搜版块，现在公司的需求是抓取实时的热搜信息，持续一周的时间。这个时候，我们最优的爬取就是增量爬，<strong>每次只爬新出现</strong> 的，或者被修改过的。这样数量越多，速度越快，后期速度就可以提升几百倍。</p><p><strong>具体举例：</strong></p><p>第一次抓取，则需要抓取全部数据。所需的抓取时间比如：<strong>12h</strong>，那等你第二天再抓取该微博热搜榜，<strong>难道要继续再重新全部抓取一遍和存储么？</strong></p><p>显然这是非常低效的，所以，这个增量式爬虫就是 <strong>只爬取微博热搜榜上新出现的数据（评论）或者哪些式被修改的数据等等</strong> 这样原本所需要 <strong>12h</strong> 的抓取时间，就比如变成 <strong>2h</strong> 实际时间。不过这个<strong>时间</strong>还是按实际情况来。（有硬件、网络、策略、实现方法等影响因素）</p><p><strong>我们先来快速简单的回顾一下：</strong></p><ol><li>请求网站资源</li><li>如果网站的给你返回一个 <strong>status_code 是200，那服务器就会给你返回那个资源</strong></li><li>然后，我们需要 <strong>Parser</strong> 来解析我们得到的资源。可以用来解析的轮子有哪些呢？ <ul><li>BeautifulSoup</li><li>PyQuery</li><li>Xpath</li><li>re （正则表达式）</li></ul></li><li>存储入库</li></ol><p><strong>实现：</strong></p><ol><li>发送请求前——判断这个 <strong>URL</strong> 是不是之前爬取过的（爬取过的 <strong>Url</strong> 我们就不再继续爬取）</li><li>解析内容后——判断这部分内容是不是之前爬取过（也就是解析得到数据之后，判断之前是否有爬取到过。实现方法就类似我们可以创建一个很大的数据库池。然后，把你解析得到的数据和里面的数据进行比对一下，看有无变化）</li><li>存储介质（数据入库时）——判断内容是不是已经在介质中存在（判断我解析得到的数据是不是在我们数据库已经出现过了，如果出现的话，我们就不入库。如果是修改过的我们就把新修改过的在重新入库，替换掉原先那个）</li></ol><p><strong>难点：其实增量式爬虫最难的那部分，就是数据的比对呢？</strong></p><ul><li>常见的增量式爬虫使用 <strong>哈希</strong></li><li>哈希可以生成<strong>独一无二的哈希数字</strong></li><li>我们只需要比对哈希即可</li></ul><p>代码示例：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># 开发团队   ：AI悦创</span>
<span class="token comment"># 开发人员   ：AI悦创</span>
<span class="token comment"># 开发时间   ：2019/9/4  8:25 </span>
<span class="token comment"># 文件名称   ：haxi_1.PY</span>
<span class="token comment"># 开发工具   ：PyCharm</span>
text <span class="token operator">=</span> <span class="token string">&quot;这是第一个评论&quot;</span>
text2 <span class="token operator">=</span> <span class="token string">&quot;这是第二个评论&quot;</span>
text3 <span class="token operator">=</span> <span class="token string">&quot;这是第一个评论&quot;</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">hash</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">hash</span><span class="token punctuation">(</span>text2<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">hash</span><span class="token punctuation">(</span>text3<span class="token punctuation">)</span><span class="token punctuation">)</span>
compare <span class="token operator">=</span> text <span class="token operator">==</span> text3
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;text == text3 is </span><span class="token interpolation"><span class="token punctuation">{</span>compare<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>

输出：
<span class="token comment"># 注意，不同编译器，不同电脑环境生成的哈希是不一样的。</span>
<span class="token comment">#	就算是同一台电脑同一个编译器，每次运行生成的哈希数字也是不一样的。</span>

<span class="token comment"># 第一次运行输出：</span>
<span class="token number">1136825877591950482</span>
<span class="token operator">-</span><span class="token number">1853188872266928196</span>
<span class="token number">1136825877591950482</span>
text <span class="token operator">==</span> text3 <span class="token keyword">is</span> <span class="token boolean">True</span>

<span class="token comment"># 第二次运行输出：</span>
<span class="token operator">-</span><span class="token number">1711655599760904659</span>
<span class="token operator">-</span><span class="token number">4125108592533926432</span>
<span class="token operator">-</span><span class="token number">1711655599760904659</span>
text <span class="token operator">==</span> text3 <span class="token keyword">is</span> <span class="token boolean">True</span>

<span class="token comment"># 第三次运行输出：</span>
<span class="token operator">-</span><span class="token number">1365827303885436725</span>
<span class="token operator">-</span><span class="token number">4683858178280503991</span>
<span class="token operator">-</span><span class="token number">1365827303885436725</span>
text <span class="token operator">==</span> text3 <span class="token keyword">is</span> <span class="token boolean">True</span>

<span class="token comment"># 这里你还会发现，虽然每次生成都会变化，但最后一行对相同的字符串做出比较还是成功的也就是（True）。</span>
<span class="token comment"># 哈希值设涉及数据结构和算法，有兴趣的学员的自行了解。</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>扩展：</p><blockquote><p>A：老师，我想选择的是标签是span class是font的，为什么上面那个class=&#39;day font&#39;也会被选择到 Teacher：标签里的class可以使用多个类，在前端多个css一起用是用空格分割的，每一个class都是独立的 B：老师python3的hash() 是不是和python2里的 hash() 不一样呀</p></blockquote><p>代码示例：</p><p>Python2：</p><p><img src="`+y+'" alt="" loading="lazy"></p><p>Python3：</p><p><img src="'+w+`" alt="" loading="lazy"></p><blockquote><p>C：好像每一次生成的都不一样 Teacher：本身就是随机的，环境不同会出现不同运算结果，算法改进了。</p></blockquote><p>那这个 <strong>hash</strong> 生成的随机数，如何来帮助我们实现 <strong>增量式爬虫呢？</strong></p><p>我例如可以，在每一条得到的数据在入库之前，哈希一下。然后可以把它的数字比如存进一个集合里面（集合：里面只能存在不同的对象）。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>再详细一些就是：当我们把第一个数据例如上面的 <strong>hash 测试代码</strong> 第一个哈希：这是第一个评论，生成的伪随机数，存到我们的集合里面，第二个哈希由：这是第二个评论，生成的伪随机数，也存到我们所创建的集合中。当第三个哈希由：这是第一个评论，生成的伪随机数，存到集合中，集合将会将他阻挡在外。</p><p>当然，你还可以使用数据库，建立一个<strong>唯一索引</strong>。</p><p>扩展：还有 <strong>指纹、布隆过滤器等（相关数学）</strong></p><h3 id="_5-2-提取信息" tabindex="-1"><a class="header-anchor" href="#_5-2-提取信息" aria-hidden="true">#</a> 5.2 提取信息</h3><p><img src="`+x+'" alt="" loading="lazy"></p><h3 id="_5-3-保存爬虫数据" tabindex="-1"><a class="header-anchor" href="#_5-3-保存爬虫数据" aria-hidden="true">#</a> 5.3 保存爬虫数据</h3><p><img src="'+q+'" alt="" loading="lazy"></p><p><img src="'+z+'" alt="" loading="lazy"></p><hr><p><strong>问题：有人可能会问为什么存到数据库里，这个数据库不也是在我电脑硬盘里面存在的吗？</strong></p><p><strong>答：虽然数据库也是装在我们的电脑里面，和我们直接存储这些：txt、world、Excel、CSV等文件，有什么区别呢？</strong></p><p><strong>其实，学过计算机原理的都懂哈。我们直接存储在硬盘里面（Disk）存储的速度是比较慢的。这是因为如下：</strong></p><blockquote><p>1.我们的操作系统，会把它压入一个高速缓冲区，之后再慢慢的刷新到我们的硬盘里面。</p></blockquote><p>相比之下，我们直接操作 <strong>CPU</strong> 就非常之快了，例如：我们的内存型数据库 <strong>Redis</strong> 就是直接存储在数据库中。（我们的 CUP 内存里面）比操作在硬盘里面的高速缓冲区，快了不止十倍。</p><p><strong>所以，Redis 现在是默认程序员掌握的。</strong></p><p><strong>但是，Redis 存储在内存有个不好的地方就是，你可能存的太多有可能把内存搞奔溃——内存溢出</strong></p><p>另一个就是，我i可能偶尔发生断电这些，电脑突然黑屏关机了。那这些数据全部消失了，所以，用 CPU 这样的方法，还是要有集群方案，和容灾措施、及时的备份、和 log（日志）</p><h2 id="_6-网络爬虫的自我约束" tabindex="-1"><a class="header-anchor" href="#_6-网络爬虫的自我约束" aria-hidden="true">#</a> 6. 网络爬虫的自我约束</h2><p>看完上面“猫捉老鼠”的游戏的描述，同学们不禁要问，网站和爬虫这种对抗较量会不会引起法律问题？</p><p><strong>这是一个很好的问题，也是值得每个爬虫开发者思考的问题。</strong></p><p><strong>爬虫作为一种技术本身可能无所谓善恶，但是使用它的人就有善恶之分。</strong></p><p>如何使用爬虫，爬取的数据如何使用，都可能产生潜在的法律问题。作为技术开发的程序员，都应该思考这个问题。无论何种目的，网络爬虫都不能突破法律的底线，同时也有遵守一定的准则：</p><ul><li>遵循 robots.txt 协议；</li><li>避免短时间高并发访问目标网站，避免干扰目标网站的正常运行；</li><li>不要抓取个人信息，比如手机通讯录等；</li><li>使用抓来的数据注意隐私保护，合法合规。</li></ul><p>守法合规，既是一直自我约束，也是自我保护。</p><h2 id="_7-我们可以抓什么数据" tabindex="-1"><a class="header-anchor" href="#_7-我们可以抓什么数据" aria-hidden="true">#</a> 7. 我们可以抓什么数据</h2><p><img src="'+P+`" alt="" loading="lazy"></p><hr><h2 id="_8-识别网站技术" tabindex="-1"><a class="header-anchor" href="#_8-识别网站技术" aria-hidden="true">#</a> 8. 识别网站技术</h2><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>pip install builtwith
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>可以检测网站的一些常见技术</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># !/usr/bin/python3</span>
<span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># @Author：AI悦创 @DateTime ：2020/1/5 17:44 @Function ：功能  Development_tool ：PyCharm</span>
<span class="token comment"># code is far away from bugs with the god animal protecting</span>
<span class="token comment">#    I love animals. They taste delicious.</span>
<span class="token comment"># 如何识别网站技术</span>
<span class="token keyword">import</span> builtwith
<span class="token keyword">from</span> pprint <span class="token keyword">import</span> pprint

<span class="token comment"># url = &#39;https://www.baidu.com/&#39;</span>
url <span class="token operator">=</span> <span class="token string">&#39;https://www.cnblogs.com/&#39;</span>
html <span class="token operator">=</span> builtwith<span class="token punctuation">.</span>builtwith<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
pprint<span class="token punctuation">(</span>html<span class="token punctuation">)</span>

<span class="token comment"># ----------------输出----------------</span>
<span class="token punctuation">{</span><span class="token string">&#39;advertising-networks&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;DoubleClick for Publishers (DFP)&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
 <span class="token string">&#39;javascript-frameworks&#39;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;jQuery&#39;</span><span class="token punctuation">]</span><span class="token punctuation">}</span>

Process finished <span class="token keyword">with</span> exit code <span class="token number">0</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h2 id="_9-网络爬虫策略" tabindex="-1"><a class="header-anchor" href="#_9-网络爬虫策略" aria-hidden="true">#</a> 9. 网络爬虫策略：</h2><p><img src="`+R+'" alt="" loading="lazy"></p><hr><h3 id="_9-1-广度优先算法" tabindex="-1"><a class="header-anchor" href="#_9-1-广度优先算法" aria-hidden="true">#</a> 9.1 广度优先算法</h3><p><img src="'+I+'" alt="" loading="lazy"></p><hr><p>比如下图是个 <strong>url 树</strong>，在程序进行 <strong>url 提取和访问的时候所提取的步骤是图中所写的编号</strong>（其实，这个也是二叉树）图中的顺序就是按广度优先算法来操作与读取的。也就是一层一层的来操作。</p><hr><p><img src="'+B+'" alt="" loading="lazy"></p><hr><p>我们用糗事百科来说：</p><p>比如我吗要爬取这个热门下面的所有 <strong>url</strong>，我使用广度优先算法（BFS）来进行抓取的话，它的顺序是：先把这个热门页面下的 <strong>所有 url 都爬取下来，我们不会特别进入每一个里面</strong>，我们就会继续爬 <strong>第二页、第三页、第四页......</strong></p><p><img src="'+E+'" alt="" loading="lazy"></p><hr><p>然后，<strong>我们再进入到单个里面把所有的 url 爬取下来。再进入到下一个爬取 url</strong>。</p><p>（这其实，就是一层一层的往下爬取）</p><p>也就是，先扩展再深入。</p><p><img src="'+L+'" alt="" loading="lazy"></p><hr><p><img src="'+U+'" alt="" loading="lazy"></p><hr><h3 id="_9-2-深度优先算法" tabindex="-1"><a class="header-anchor" href="#_9-2-深度优先算法" aria-hidden="true">#</a> 9.2 深度优先算法</h3><p><img src="'+A+'" alt="" loading="lazy"></p><hr><p>可以理解为：一条路走到黑（不撞南墙不回头）</p><p>我们也再一次的来使用 <strong>二叉树来讲解</strong>我们的深度优先算法，下图所标的序号就是我们所说的深度优先算法。</p><p><img src="'+D+'" alt="" loading="lazy"></p><h3 id="_9-3-聚焦爬虫" tabindex="-1"><a class="header-anchor" href="#_9-3-聚焦爬虫" aria-hidden="true">#</a> 9.3 聚焦爬虫</h3><p><img src="'+O+'" alt="" loading="lazy"></p><hr><p>相对于通用网络爬虫，聚焦爬虫解决三个主要问题：</p><ol><li>对抓取目标的描述或定义；</li><li>对网页或数据的分析与过滤；</li><li>对 URL 的搜索策略。</li></ol><blockquote><p>抓取目标的描述和定义是决定网页分析算法与 URL 搜索策略如何制定的基础。 而网页分析算法和候选 URL 排序算法是决定搜索引擎所提供的服务形式和爬虫网页抓取行为的关键所在。这两个部分的算法又是紧密相关的。</p></blockquote><p><strong>制定 URL 搜索策略：</strong></p><p>当然除了上面讲的：BFS（广度优先）、DFS（深度优先），我们不经常使用这么简单的爬虫策略了。</p><p>例如：<strong>按优先级的爬虫策略</strong></p><blockquote><p>比如我们现在要爬取豆瓣上的书籍内容，所以，比方说我们现在要爬取的网站 url 是：<strong>/var/page=1</strong>、<strong>/view/page=1</strong>这样的 <strong>url</strong> 我们该怎么处理呢？ 我们对这些 url 做优先级划分，比方说 <strong>用 var 开头的，我们就放在列表前面、以 view 开头我们就放在列表的后面</strong>。优先爬取与我们 <strong>var</strong> 相关的。——这就是我们所说的优先级</p></blockquote><p>我们都会对 url 进行分析的。</p><hr><h2 id="_10-反爬虫案例" tabindex="-1"><a class="header-anchor" href="#_10-反爬虫案例" aria-hidden="true">#</a> 10. 反爬虫案例</h2><p><img src="'+S+'" alt="" loading="lazy"></p><hr><blockquote><p>PS：**Ajax 交互：**在我们访问一个网站的时候，页面在不断的刷新，而我们的 url 没有变化。（就像百度图片这样的就是 Ajax 交互。）</p></blockquote><h3 id="_10-1-referer" tabindex="-1"><a class="header-anchor" href="#_10-1-referer" aria-hidden="true">#</a> 10.1 Referer</h3><p>**referer：**防盗链，作用就是你次的这次请求是从哪里来的。</p><p><strong>我们一一来看一看：</strong></p><p><strong>Headers 检测</strong></p><p>下面的 <strong>iter_content(512)</strong> 小块读取（第一篇就是大揽，之后都会讲到）</p><p><img src="'+T+`" alt="" loading="lazy"></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> requests
url <span class="token operator">=</span> <span class="token string">&#39;http://img3.laibafile.cn/p/m/310277586.jpg&#39;</span>
headers <span class="token operator">=</span> <span class="token punctuation">{</span>
	<span class="token string">&#39;Referer&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;http://bbs.tianya.cn/pic-no04-2829676.shtml&#39;</span>
<span class="token comment">#我上一次请求在哪里（或者说，我发起下载这个图片的请求是从哪个地方发起的。</span>
<span class="token punctuation">}</span>
img <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers <span class="token operator">=</span> headers<span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;refer.jpg&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;wb&#39;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
	f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>img<span class="token punctuation">.</span>content<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+C+`" alt="" loading="lazy"></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> requests
url <span class="token operator">=</span> <span class="token string">&#39;http://img3.laibafile.cn/p/m/310277586.jpg&#39;</span>
<span class="token comment"># headers = {</span>
<span class="token comment"># 	&#39;Referer&#39;: &#39;http://bbs.tianya.cn/pic-no04-2829676.shtml&#39;</span>
<span class="token comment">#我上一次请求在哪里（或者说，我发起下载这个图片的请求是从哪个地方发起的。</span>
<span class="token comment">#</span>
<span class="token comment"># }</span>
<span class="token comment"># img = requests.get(url, headers = headers)</span>
img <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&#39;refer.jpg&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;wb&#39;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
	f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>img<span class="token punctuation">.</span>content<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+j+'" alt="" loading="lazy"></p><hr><h3 id="_10-2-用户访问频率限制-这个刚刚开始只需要了解即可-涉及异步" tabindex="-1"><a class="header-anchor" href="#_10-2-用户访问频率限制-这个刚刚开始只需要了解即可-涉及异步" aria-hidden="true">#</a> 10.2 用户访问频率限制（这个刚刚开始只需要了解即可-涉及异步）</h3><p><img src="'+W+'" alt="" loading="lazy"></p><blockquote><p>以上代码是异步爬虫，然后上面的图片就是为了加快速度让豆瓣封我 <strong>IP</strong> 代码实现都不用疑惑，之后都会讲到滴。爬取到了许多资源。 如何，防止被封 <strong>IP</strong> 第一个使用： <strong>IP</strong>，第二个就是：<strong>降低爬虫速度。</strong></p></blockquote><h3 id="_10-3-用户延迟访问插件" tabindex="-1"><a class="header-anchor" href="#_10-3-用户延迟访问插件" aria-hidden="true">#</a> 10.3 用户延迟访问插件</h3>',119),sn=e('<p><img src="'+F+'" alt="" loading="lazy"></p><p>有基础的你，有可能会有疑惑。为什么不使用 <strong>time.sleep(sum)</strong> 我可以明确告诉你，可以使用。但是，会造成时间浪费。</p><h3 id="_10-4-ajax-交互" tabindex="-1"><a class="header-anchor" href="#_10-4-ajax-交互" aria-hidden="true">#</a> 10.4 Ajax 交互</h3><p>举个例子就是百度图片，这个你直接 <strong>Get</strong> 是请求不到的，之后会有找 <strong>Json 接口、API 构造一些参数来</strong></p><p><img src="'+Q+'" alt="" loading="lazy"></p><h3 id="_10-5-网页数据加密" tabindex="-1"><a class="header-anchor" href="#_10-5-网页数据加密" aria-hidden="true">#</a> 10.5 网页数据加密</h3><p><img src="'+G+'" alt="" loading="lazy"></p><h3 id="_10-6-验证码" tabindex="-1"><a class="header-anchor" href="#_10-6-验证码" aria-hidden="true">#</a> 10.6 验证码</h3><p><img src="'+J+'" alt="" loading="lazy"></p><h3 id="_10-7-字体反爬虫" tabindex="-1"><a class="header-anchor" href="#_10-7-字体反爬虫" aria-hidden="true">#</a> 10.7 字体反爬虫</h3><p><img src="'+V+'" alt="" loading="lazy"></p><p>字体反爬虫最近也是比较流行的。往期教程有讲到实习僧的网站的抓取。</p><h3 id="_10-8-用户延迟访问插件使用演示" tabindex="-1"><a class="header-anchor" href="#_10-8-用户延迟访问插件使用演示" aria-hidden="true">#</a> 10.8 用户延迟访问插件使用演示</h3><p><img src="'+N+`" alt="" loading="lazy"></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> urllib <span class="token keyword">import</span> parse
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime
<span class="token keyword">import</span> time<span class="token punctuation">,</span>requests
<span class="token keyword">class</span> <span class="token class-name">DelayWait</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> delay <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># delay = 3 ,每次延迟 3 秒；</span>
		self<span class="token punctuation">.</span>delay <span class="token operator">=</span> delay
		self<span class="token punctuation">.</span>urls <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token keyword">def</span> <span class="token function">wait</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> url<span class="token punctuation">)</span><span class="token punctuation">:</span>
		netloc <span class="token operator">=</span> parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>netloc
        <span class="token comment"># print(netloc)</span>
		lastOne <span class="token operator">=</span> self<span class="token punctuation">.</span>urls<span class="token punctuation">.</span>get<span class="token punctuation">(</span>netloc<span class="token punctuation">)</span>
        <span class="token comment"># print(lastOne)</span>

		<span class="token keyword">if</span> lastOne <span class="token keyword">and</span> self<span class="token punctuation">.</span>delay<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
			timeWait <span class="token operator">=</span> self<span class="token punctuation">.</span>delay <span class="token operator">-</span> <span class="token punctuation">(</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>lastOne<span class="token punctuation">)</span><span class="token punctuation">.</span>seconds
			<span class="token keyword">if</span> timeWait<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
				time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>timeWait<span class="token punctuation">)</span>

		self<span class="token punctuation">.</span>urls<span class="token punctuation">[</span>netloc<span class="token punctuation">]</span> <span class="token operator">=</span> datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span>

urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;http://www.baidu.com&#39;</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">10</span>
d <span class="token operator">=</span> DelayWait<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
	html <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	d<span class="token punctuation">.</span>wait<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span>html<span class="token punctuation">.</span>status_code<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>同学理解：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> urllib <span class="token keyword">import</span> parse
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime
<span class="token keyword">import</span> time<span class="token punctuation">,</span>requests
<span class="token keyword">class</span> <span class="token class-name">DelayRequests</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>delay<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#定义延迟类</span>
        self<span class="token punctuation">.</span>delay<span class="token operator">=</span>delay<span class="token comment">#类的属性,类的延迟等于3</span>
        self<span class="token punctuation">.</span>urls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#链接</span>
    <span class="token keyword">def</span> <span class="token function">wait</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>url<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#定义类的方法：延迟方法</span>
        netloc<span class="token operator">=</span>parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>netloc
        <span class="token comment">#print(netloc)  输出域名，netloc是域名的意思 ,这里不是很确定</span>
        lastOne<span class="token operator">=</span>self<span class="token punctuation">.</span>urls<span class="token punctuation">.</span>get<span class="token punctuation">(</span>netloc<span class="token punctuation">)</span>
        <span class="token comment">#print(lastOne),获取最后一个域名对应的链接</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>delay<span class="token operator">&gt;</span><span class="token number">0</span> <span class="token keyword">and</span> lastOne<span class="token punctuation">:</span><span class="token comment">#延时设置</span>
            sleepTime<span class="token operator">=</span>self<span class="token punctuation">.</span>delay<span class="token operator">-</span>\\
                        <span class="token punctuation">(</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>lastOne<span class="token punctuation">)</span><span class="token punctuation">.</span>seconds<span class="token comment">#3与两个链接分钟差值进行运算，其中&#39;-\\&#39;不理解，李老师说的难度可能在这</span>
            <span class="token keyword">if</span> sleepTime<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
                time <span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>sleepTime<span class="token punctuation">)</span><span class="token comment">#修改sleep时间，避免课程中说的时间浪费</span>
        self<span class="token punctuation">.</span>urls<span class="token punctuation">[</span>netloc<span class="token punctuation">]</span><span class="token operator">=</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#得到当前时间即上一延迟的后的链接，从而把它当初下一延迟的初始链接</span>
urls<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&#39;https://blog.csdn.net/&#39;</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">10</span><span class="token comment">#运用</span>
d<span class="token operator">=</span>DelayRequests<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#实例化</span>
<span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
    html<span class="token operator">=</span>requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
    d<span class="token punctuation">.</span>wait<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token comment">#链接延时</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>html<span class="token punctuation">.</span>status_code<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>PS：这么理解即可</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> urllib <span class="token keyword">import</span> parse <span class="token comment"># 解析 URL</span>
<span class="token keyword">from</span> datetime <span class="token keyword">import</span> datetime  <span class="token comment"># 获取时间（标注时间）时间加减</span>
<span class="token keyword">import</span> time<span class="token punctuation">,</span>requests <span class="token comment"># time 实现睡眠 # requests  爬虫库</span>

<span class="token comment"># 一般插件的功能是很专一的，当个文件只实现一个功能</span>
<span class="token keyword">class</span> <span class="token class-name">DelayWait</span><span class="token punctuation">:</span>
	<span class="token comment"># 内置函数初始化： __init__</span>
	<span class="token comment"># 一般来说，在我们实行这个类的的时候，就会自动执行这个类的初始化函数（它是第一个执行的）</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> delay <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># 初始化参数（初始化属性，属性：对象的某个静态特征）</span>
		<span class="token comment"># delay = 3 ,每次延迟 3 秒；</span>
		self<span class="token punctuation">.</span>delay <span class="token operator">=</span> delay <span class="token comment"># 延迟时间</span>
		self<span class="token punctuation">.</span>urls <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 存储各种 URL</span>

	<span class="token comment"># 函数：对象的某个动态能力</span>
	<span class="token keyword">def</span> <span class="token function">wait</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> url<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># 解析我们的 URL，来对比每次访问的主站，是否是同一个主站，同一个就是使用该延迟插件，不是就不用啦！</span>
		<span class="token comment"># 因为，我们封 IP 其实就是，快速重复访问同一个网站,才有可能被封</span>
		netloc <span class="token operator">=</span> parse<span class="token punctuation">.</span>urlparse<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">.</span>netloc

		<span class="token comment"># 我们上一步请求的网址是什么，看有没有这个主站。</span>
		<span class="token comment"># 这里的 get(&#39;&#39;, null) 是字典的一个方法，使用get来查询字典中的数据，如果这个数据存在，则返回改键对应的值。</span>
		<span class="token comment"># 不存在则返回：预先设定的内容：null</span>
		lastOne <span class="token operator">=</span> self<span class="token punctuation">.</span>urls<span class="token punctuation">.</span>get<span class="token punctuation">(</span>netloc<span class="token punctuation">)</span>

		<span class="token comment"># 如果，我们上一次请求过的话，就执行这个语句</span>
		<span class="token keyword">if</span> lastOne <span class="token keyword">and</span> self<span class="token punctuation">.</span>delay<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
			<span class="token comment"># 本次访问和上一次访问的时间差.</span>
			timeWait <span class="token operator">=</span> self<span class="token punctuation">.</span>delay <span class="token operator">-</span> <span class="token punctuation">(</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>lastOne<span class="token punctuation">)</span><span class="token punctuation">.</span>seconds
			<span class="token comment"># seconds 转换为秒</span>
			<span class="token comment"># 解析:</span>
			<span class="token comment"># 	如果两次请求的时间差(datetime.now()-lastOne)非常小,表明,两次请求的时间间隔,非常小.</span>
			<span class="token comment">#   所以,以每次请求 delay 所设定的时间为标准.如果得出来的差为 3s(举例)那就不执行延迟,</span>
			<span class="token comment">#   如果差小于 3s 则执行与 3s 相差的时间的差.</span>
			<span class="token keyword">if</span> timeWait<span class="token operator">&gt;</span><span class="token number">0</span><span class="token punctuation">:</span>
				time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span>timeWait<span class="token punctuation">)</span>

		<span class="token comment"># 为字典 urls 添加：键对值：主站:对应添加的时间</span>
		self<span class="token punctuation">.</span>urls<span class="token punctuation">[</span>netloc<span class="token punctuation">]</span> <span class="token operator">=</span> datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span>

urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;http://www.baidu.com&#39;</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">10</span>
d <span class="token operator">=</span> DelayWait<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
	html <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	d<span class="token punctuation">.</span>wait<span class="token punctuation">(</span>url<span class="token punctuation">)</span>
	<span class="token keyword">print</span><span class="token punctuation">(</span>html<span class="token punctuation">.</span>status_code<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="`+H+'" alt="" loading="lazy"></p><p>欢迎关注我公众号：AI悦创，有更多更好玩的等你发现！</p><details class="custom-container details"><summary>公众号：AI悦创【二维码】</summary><p><img src="'+l+'" alt="" loading="lazy"></p></details>',22),an={class:"custom-container info"},tn=n("p",{class:"custom-container-title"},"AI悦创·编程一对一",-1),en=n("p",null,"AI悦创·推出辅导班啦，包括「Python 语言辅导班、C++ 辅导班、java 辅导班、算法/数据结构辅导班、少儿编程、pygame 游戏开发」，全部都是一对一教学：一对一辅导 + 一对一答疑 + 布置作业 + 项目实践等。当然，还有线下线上摄影课程、Photoshop、Premiere 一对一教学、QQ、微信在线，随时响应！微信：Jiabcdefh",-1),pn=n("p",null,"C++ 信息奥赛题解，长期更新！长期招收一对一中小学信息奥赛集训，莆田、厦门地区有机会线下上门，其他地区线上。微信：Jiabcdefh",-1),on={href:"http://wpa.qq.com/msgrd?v=3&uin=1432803776&site=qq&menu=yes",target:"_blank",rel:"noopener noreferrer"},ln=n("p",null,"方法二：微信：Jiabcdefh",-1),cn=n("p",null,[n("img",{src:i,alt:"",loading:"lazy"})],-1);function rn(un,dn){const t=p("ExternalLinkIcon"),o=p("RouterLink");return r(),u("div",null,[$,n("blockquote",null,[n("p",null,[s("暗网（不可见网，隐藏网）是指那些存储在"),n("a",X,[s("网络"),a(t)]),s("数据库里、但不能通过"),n("a",Y,[s("超链接"),a(t)]),s("访问而需要通过动态网页技术访问的资源集合，不属于那些可以被标准搜索引擎索引的表面网络。 暗网是"),n("a",K,[s("深网"),a(t)]),s("（Deep Web）的一个子集，属于深网的一小部分。 迈克尔·伯格曼将当今互联网上的搜索服务比喻为像在地球的海洋表面的拉起一个大网的搜索，大量的表面信息固然可以通过这种方式被查找得到，可是还有相当大量的信息由于隐藏在深处而被搜索引擎错失掉。绝大部分这些隐藏的信息是须通过动态请求产生的网页信息，而标准的搜索引擎却无法对其进行查找。传统的搜索引擎“看”不到，也获取不了这些存在于暗网的内容，除非通过特定的搜查这些页面才会动态产生。于是相对的，暗网就隐藏了起来。 "),Z])]),nn,n("p",null,[s("该插件详细解析："),a(o,{to:"/column/crawler/replenish02.html"},{default:d(()=>[s("还没用异步，爬取就速度太快？更智能的延迟插件")]),_:1})]),sn,n("div",an,[tn,en,pn,n("p",null,[s("方法一："),n("a",on,[s("QQ"),a(t)])]),ln]),cn])}const gn=c(M,[["render",rn],["__file","s2_02.html.vue"]]);export{gn as default};
