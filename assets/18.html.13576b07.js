import{_ as e}from"./gzh.b03adfd2.js";import{_ as t}from"./zsxq.77f6dc86.js";import{_ as p}from"./_plugin-vue_export-helper.cdc0426e.js";import{o,c,a as s,d as n,b as i,e as l,r}from"./app.d146ebf6.js";const d="/assets/e7ebf8c2cbda7afae59fcac0d9314b7f.f163ece1.jpg",u="/assets/af89317aa55ac3b9f068b0f370fcb9cf.14a6dc31.png",m="/assets/f9bb4cce5b895499cabc714eb372b089.361de2da.png",A="/assets/69a90a43146898150a0de0811c6fef9a.8276db61.jpg",k="/assets/107fed838cb75df62eb149499db20c1e.2a008504.png",v="/assets/aa423c65b32bded13212b7e20fb65a0c.dbe5287a.png",g="/assets/092a0ea87aabc5da482ff8a992691b77.435a6078.png",b="/assets/3c08d5cd66a8ea098c397e14f1469ff8.183047d5.png",f="/assets/c1e2f9e4a299789bb6cc23afc6fd3140.fccbc689.png",h="data:image/png;base64,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",C="/assets/045fd5afb7b53f17a8accd6f337f63c1.6d074b77.png",y="/assets/65a3855aed648b32994b808296a40b61.11b6452e.png",Q={},_=l('<p><img src="'+d+'" alt="img" loading="lazy"></p><p>你好，我是悦创。</p><p>上节课我们讲了决策树，基于信息度量的不同方式，我们可以把决策树分为 ID3 算法、C4.5 算法和 CART 算法。今天我来带你学习 CART 算法。CART 算法，英文全称叫做 Classification And Regression Tree，中文叫做分类回归树。ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。</p><p>那么你首先需要了解的是，什么是分类树，什么是回归树呢？</p><p>我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。</p><p><img src="'+u+'" alt="img" loading="lazy"></p><p>分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p><h2 id="cart-分类树的工作流程" tabindex="-1"><a class="header-anchor" href="#cart-分类树的工作流程" aria-hidden="true">#</a> CART 分类树的工作流程</h2><p>通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。</p><p>在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。</p><p>你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。</p><p>基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。</p><p>我们接下来详解了解一下基尼系数。基尼系数不好懂，你最好跟着例子一起手动计算下。</p><p>假设 t 为节点，那么该节点的 GINI 系数的计算公式为：</p><p><img src="'+m+'" alt="img" loading="lazy"></p><p>这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。</p><p>通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：</p><p>集合 1：6 个都去打篮球；</p><p>集合 2：3 个去打篮球，3 个不去打篮球。</p><p>针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。</p><p>针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，<code>p(C1|t)=0.5</code>，<code>p(C2|t)=0.5</code>，<code>GINI(t)=1-（0.5*0.5+0.5*0.5）=0.5</code>。</p><p>通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。</p><p>在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：</p><p><img src="'+A+'" alt="img" loading="lazy"></p><p>节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：</p><p><img src="'+k+'" alt="img" loading="lazy"></p><p>归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。</p><p>上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：</p><p><img src="'+v+'" alt="img" loading="lazy"></p><p><img src="'+g+'" alt="img" loading="lazy"></p><p>所以在属性 A 的划分下，节点 D 的基尼系数为：</p><p><img src="'+b+`" alt="img" loading="lazy"></p><p>节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。</p><h2 id="如何使用-cart-算法来创建分类树" tabindex="-1"><a class="header-anchor" href="#如何使用-cart-算法来创建分类树" aria-hidden="true">#</a> 如何使用 CART 算法来创建分类树</h2><p>通过上面的讲解你可以知道，CART 分类树实际上是基于基尼系数来做属性划分的。在 Python 的 sklearn 中，如果我们想要创建 CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。</p><p>下面，我们来用 CART 分类树，给 iris 数据集构造一棵分类决策树。iris 这个数据集，我在 Python 可视化中讲到过，实际上在 sklearn 中也自带了这个数据集。基于 iris 数据集，构造 CART 分类树的代码如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># @Time    : 2022/10/27 16:20</span>
<span class="token comment"># @Author  : AI悦创</span>
<span class="token comment"># @Software: PyCharm</span>
<span class="token comment"># @Blog    ：https://bornforthis.cn/</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris

<span class="token comment"># 准备数据集</span>
iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 获取特征集和分类标识</span>
features <span class="token operator">=</span> iris<span class="token punctuation">.</span>data
labels <span class="token operator">=</span> iris<span class="token punctuation">.</span>target
<span class="token comment"># 随机抽取33%的数据作为测试集，其余为训练集</span>
train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> test_labels <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.33</span><span class="token punctuation">,</span>
                                                                            random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># 创建CART分类树</span>
clf <span class="token operator">=</span> DecisionTreeClassifier<span class="token punctuation">(</span>criterion<span class="token operator">=</span><span class="token string">&#39;gini&#39;</span><span class="token punctuation">)</span>
<span class="token comment"># 拟合构造CART分类树</span>
clf <span class="token operator">=</span> clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span>
<span class="token comment"># 用CART分类树做预测</span>
test_predict <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span>
<span class="token comment"># 预测结果与测试集结果作比对</span>
score <span class="token operator">=</span> accuracy_score<span class="token punctuation">(</span>test_labels<span class="token punctuation">,</span> test_predict<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;CART分类树准确率 %.4lf&quot;</span> <span class="token operator">%</span> score<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>CART分类树准确率 <span class="token number">0.9600</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>如果我们把决策树画出来，可以得到下面的图示：</p><p><img src="`+f+'" alt="img" loading="lazy"></p><p>首先 train_test_split 可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。</p><p>使用 <code>clf = DecisionTreeClassifier(criterion=&#39;gini&#39;)</code> 初始化一棵 CART 分类树。这样你就可以对 CART 分类树进行训练。</p><p>使用 <code>clf.fit(train_features, train_labels)</code> 函数，将训练集的特征值和分类标识作为参数进行拟合，得到 CART 分类树。</p><p>使用 <code>clf.predict(test_features)</code> 函数进行预测，传入测试集的特征值，可以得到测试结果 test_predict。</p><p>最后使用 <code>accuracy_score(test_labels, test_predict)</code> 函数，传入测试集的预测结果与实际的结果作为参数，得到准确率 score。</p><p>我们能看到 sklearn 帮我们做了 CART 分类树的使用封装，使用起来还是很方便的。</p><h2 id="cart-回归树的工作流程" tabindex="-1"><a class="header-anchor" href="#cart-回归树的工作流程" aria-hidden="true">#</a> CART 回归树的工作流程</h2><p>CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。</p><p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为 u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p><p>其中差值的绝对值为样本值减去样本均值的绝对值：</p><p><img src="'+h+'" alt="img" loading="lazy"></p><p>方差为每个样本值减去样本均值的平方和除以样本个数：</p><p><img src="'+C+`" alt="img" loading="lazy"></p><p>所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p><p>我们可以通过一个例子来看下如何创建一棵 CART 回归树来做预测。</p><h2 id="如何使用-cart-回归树做预测" tabindex="-1"><a class="header-anchor" href="#如何使用-cart-回归树做预测" aria-hidden="true">#</a> 如何使用 CART 回归树做预测</h2><p>这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。</p><p>根据这些指标，我们使用 CART 回归树对波士顿房价进行预测，代码如下：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># -*- coding: utf-8 -*-</span>
<span class="token comment"># @Time    : 2022/10/27 16:38</span>
<span class="token comment"># @Author  : AI悦创</span>
<span class="token comment"># @Software: PyCharm</span>
<span class="token comment"># @Blog    ：https://bornforthis.cn/</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> r2_score<span class="token punctuation">,</span> mean_absolute_error<span class="token punctuation">,</span> mean_squared_error
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeRegressor

<span class="token comment"># 准备数据集</span>
boston <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 探索数据</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>boston<span class="token punctuation">.</span>feature_names<span class="token punctuation">)</span>
<span class="token comment"># 获取特征集和房价</span>
features <span class="token operator">=</span> boston<span class="token punctuation">.</span>data
prices <span class="token operator">=</span> boston<span class="token punctuation">.</span>target
<span class="token comment"># 随机抽取33%的数据作为测试集，其余为训练集</span>
train_features<span class="token punctuation">,</span> test_features<span class="token punctuation">,</span> train_price<span class="token punctuation">,</span> test_price <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>features<span class="token punctuation">,</span> prices<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.33</span><span class="token punctuation">)</span>
<span class="token comment"># 创建CART回归树</span>
dtr <span class="token operator">=</span> DecisionTreeRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 拟合构造CART回归树</span>
dtr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> train_price<span class="token punctuation">)</span>
<span class="token comment"># 预测测试集中的房价</span>
predict_price <span class="token operator">=</span> dtr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span>
<span class="token comment"># 测试集的结果评价</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;回归树二乘偏差均值:&#39;</span><span class="token punctuation">,</span> mean_squared_error<span class="token punctuation">(</span>test_price<span class="token punctuation">,</span> predict_price<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;回归树绝对值偏差均值:&#39;</span><span class="token punctuation">,</span> mean_absolute_error<span class="token punctuation">(</span>test_price<span class="token punctuation">,</span> predict_price<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果（每次运行结果可能会有不同）：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token punctuation">[</span><span class="token string">&#39;CRIM&#39;</span> <span class="token string">&#39;ZN&#39;</span> <span class="token string">&#39;INDUS&#39;</span> <span class="token string">&#39;CHAS&#39;</span> <span class="token string">&#39;NOX&#39;</span> <span class="token string">&#39;RM&#39;</span> <span class="token string">&#39;AGE&#39;</span> <span class="token string">&#39;DIS&#39;</span> <span class="token string">&#39;RAD&#39;</span> <span class="token string">&#39;TAX&#39;</span> <span class="token string">&#39;PTRATIO&#39;</span> <span class="token string">&#39;B&#39;</span> <span class="token string">&#39;LSTAT&#39;</span><span class="token punctuation">]</span>
回归树二乘偏差均值<span class="token punctuation">:</span> <span class="token number">23.80784431137724</span>
回归树绝对值偏差均值<span class="token punctuation">:</span> <span class="token number">3.040119760479042</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果把回归树画出来，可以得到下面的图示（波士顿房价数据集的指标有些多，所以树比较大）：</p><p><img src="`+y+'" alt="img" loading="lazy"></p><p>欢迎关注我公众号：AI悦创，有更多更好玩的等你发现！</p><details class="custom-container details"><summary>公众号：AI悦创【二维码】</summary><p><img src="'+e+'" alt="" loading="lazy"></p></details>',66),T={class:"custom-container info"},I=s("p",{class:"custom-container-title"},"AI悦创·编程一对一",-1),E=s("p",null,"AI悦创·推出辅导班啦，包括「Python 语言辅导班、C++ 辅导班、java 辅导班、算法/数据结构辅导班、少儿编程、pygame 游戏开发」，全部都是一对一教学：一对一辅导 + 一对一答疑 + 布置作业 + 项目实践等。当然，还有线下线上摄影课程、Photoshop、Premiere 一对一教学、QQ、微信在线，随时响应！微信：Jiabcdefh",-1),R=s("p",null,"C++ 信息奥赛题解，长期更新！长期招收一对一中小学信息奥赛集训，莆田、厦门地区有机会线下上门，其他地区线上。微信：Jiabcdefh",-1),x={href:"http://wpa.qq.com/msgrd?v=3&uin=1432803776&site=qq&menu=yes",target:"_blank",rel:"noopener noreferrer"},D=s("p",null,"方法二：微信：Jiabcdefh",-1),w=s("p",null,[s("img",{src:t,alt:"",loading:"lazy"})],-1);function B(H,G){const a=r("ExternalLinkIcon");return o(),c("div",null,[_,s("div",T,[I,E,R,s("p",null,[n("方法一："),s("a",x,[n("QQ"),i(a)])]),D]),w])}const M=p(Q,[["render",B],["__file","18.html.vue"]]);export{M as default};
